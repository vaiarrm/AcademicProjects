###author: Vaibhav Sharma#Date: 10/08/2016#Description#Arguments:#  input > a matrix with a column for each example, and a row for each input feature#  target > a matrix with a column for each example, and a row for each output feature#  nodeLayers > a vector with the number of nodes in each layer (including the input and output layers).#  numEpochs > (scalar) desired number of epochs to run#  batchSize > (scalar) number of instances in a mini-batch#  eta > (scalar) learning rate#Output: Prints the output accuracy##function start(input,target,nodeLayers,numEpochs,batchSize,eta,transferFuncName,                costFuncName,lambdaReg,mom,splitInfo=[80,10,10],oldWeights=0,oldBiases=0)          # Global varaibles that will be shared across multiple functions  global weights  global biases  global numberOfLayers  global cost  global correct  global transferFunc  global transferFuncDer  global correctCal  global costFunc  global calcLastDeltaVal      # Initializing Global Varaibles  numberOfLayers = columns(nodeLayers);  # Setting Up Funcitons  if strcmp(transferFuncName,"sigmoid")    transferFunc = @sigmoid;    transferFuncDer = @sigmoid_prime;    correctCal=@round_correctCal;      elseif strcmp(transferFuncName,"relu")    transferFunc = @relu;    transferFuncDer = @relu_prime;    correctCal=@round_correctCal;      elseif strcmp(transferFuncName,"tanh")    transferFunc = @tanhTransfer;    transferFuncDer = @tanhTransfer_prime;    correctCal=@round_correctCal;      elseif strcmp(transferFuncName,"softmax")    transferFunc = @softmax;    transferFuncDer = @softmax_prime;    correctCal=@softmax_correctCal;      endif    if strcmp(costFuncName,"cross")    costFunc = @cross_entropy;    calcLastDeltaVal = @deltaValForLogAndCross;      elseif strcmp(costFuncName,"quad")    costFunc = @quadratic;    calcLastDeltaVal = @deltaValForQuadratic;      elseif strcmp(costFuncName,"log")    costFunc = @log_cost;    calcLastDeltaVal = @deltaValForLogAndCross;      endif    if oldWeights == 0 && oldBiases == 0    weights = cell(1,numberOfLayers);    biases  = cell(1,numberOfLayers);    for i = 2: (numberOfLayers)      numNeuronInCurrentLayer = nodeLayers(i);      numNeuronInPrevLayer = nodeLayers(i-1);      weights(i) = randn(numNeuronInCurrentLayer,numNeuronInPrevLayer) ./ sqrt(numNeuronInPrevLayer);       biases(i) = randn(numNeuronInCurrentLayer,1);    endfor  else    weights = oldWeights;    biases = oldBiases;  endif    trainAcc = ones(1,numEpochs);  trainCostList = ones(1,numEpochs);  validAcc = ones(1,numEpochs);  validCostList = zeros(1,numEpochs);  testAcc = ones(1,numEpochs);  testCostList = ones(1,numEpochs);    earlyStopTestStart = round(numEpochs * 33.33/100);  noOfPrevEpochsToCheck = round(numEpochs * 20/100);  printf("      |            TRAIN            ||              TEST         ||         VALIDATION      \n")  printf("____________________________________________________________________________________________\n")  printf("  Ep  | Cost  |    Corr   |  Acc    ||  Cost |    Corr   |  Acc  ||  Cost |    Corr   |  Acc\n")  printf("____________________________________________________________________________________________\n")    for i = 1:numEpochs    # Shuffling data    [trainInput,trainTarget,validInput,validTarget,testInput,testTarget] = shuffle(input,target,splitInfo);        cost = 0.0;    correct = 0;    for j = 1:batchSize:size(trainInput,2)      a = trainInput(:,[j:j+batchSize-1]);      t = trainTarget(:,[j:j+batchSize-1]);      backpropogation(a,t,batchSize,eta,lambdaReg,mom,size(trainInput,2));    endfor    trainingCost = cost ;    trainingCorrect = correct;        # Calculating Validation Accuracy    validCost = 0.0;    validCorrect = 0;       z = 0;    a = 0;    for k = 1:size(validInput,2)      a = validInput(:,[k:k]);      t = validTarget(:,[k:k]);       for l = 2:numberOfLayers        w = weights{1,l};        b = biases{1,l};        z = w * a + b;        a = transferFunc(z,l);      endfor      validCost = validCost + costFunc(t,a,lambdaReg);        validCorrect = validCorrect + correctCal(a,t);    endfor    # Calculating Test Accuracy    testCost = 0.0;    testCorrect = 0;    z = 0;    a = 0;        for k = 1:size(testInput,2)      a = testInput(:,[k:k]);      t = testTarget(:,[k:k]);       for l = 2:numberOfLayers        w = weights{1,l};        b = biases{1,l};        z = w * a + b;        a = transferFunc(z,l);      endfor      testCost = testCost + costFunc(t,a,lambdaReg);        testCorrect = testCorrect + correctCal(a,t);    endfor            trainAcc(i) = trainingCorrect / size(trainInput,2);    trainCostList(i) = trainingCost/ size(trainInput,2);        validAcc(i) = validCorrect / size(validInput,2);    validCostList(i) = validCost/size(validInput,2);        testAcc(i) = testCorrect / size(testInput,2);    testCostList(i) = testCost /  size(testInput,2);            printf("%-4d  | %-3.3f | %4d/%-4d | %-0.3f   || %-0.3f | %4d/%-4d | %-0.3f || %-0.3f | %4d/%-4d | %-0.3f\n",    i,trainCostList(i),trainingCorrect,size(trainInput,2),trainAcc(i),testCostList(i),testCorrect,size(testInput,2),testAcc(i),validCostList(i),    validCorrect,size(validInput,2),validAcc(i));        #EARLY STOP CRITERIA        #if trainAcc(i) == 1    #  break    #endif    if numEpochs >= 10      if i >= earlyStopTestStart        inc = 0;        dec = 0;        cons = 0;        for e = i:-1:i-noOfPrevEpochsToCheck          perChange = (validCostList(i) - validCostList(i-1))* 100;# / validCostList(i-1);          #if perChange < 0        endfor      endif    endif         endfor   plotAccuraciesAndCosts(trainAcc,validAcc,testAcc,numEpochs,trainCostList,validCostList,testCostList)  #plotCosts(trainCostList,validCostList,testCostList,numEpochs)endfunction